{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SubSimGPT2Interactive simpletransformers GPT-2 Finetuning Notebook",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm3CFElTWL7r"
      },
      "source": [
        "# Install simpletransformers Python package\n",
        "\n",
        "Simpletransformers is a package that uses pytorch to finetune GPT-2 models. It's mostly plug and play.\n",
        "\n",
        "Installing it will upgrade the colab iPython version and require the colab instance to be restarted (Runtime menu > Restart Menu Ctrl-M). Not sure how to get around that at the moment, sorry.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eMkX7AoVlpG"
      },
      "source": [
        "# Install simpletransformers\n",
        "!python -m pip install git+https://github.com/zacc/simpletransformers.git@01ed37e471234ec3266fda2101ce61f4e88e47bb\n",
        "\n",
        "# Output which type of GPU Colab has bestowed upon us\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPosJ7XTX1FW"
      },
      "source": [
        "# Set filename of finetuning data files\n",
        "Set the variables with the name of the training data text file.\n",
        "\n",
        "There are two data files: training file and the evaluation file. The training file is what GPT-2 is fine tuned with and the evaluation file is used by simpletransformers as a control sample to compare how the fine tuning progress is going.\n",
        "\n",
        "Generally this is done by taking your training data and splitting it into parts of 90%/10%; 90% for the training data and 10% for the evaluation data.\n",
        "\n",
        "These data files should be copied to the root directory of your Google Drive and it will read the file directly from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtCFofMoYYtY"
      },
      "source": [
        "training_file = \"bot_27102020_train.txt\"\n",
        "eval_file = \"bot_27102020_eval.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE"
      },
      "source": [
        "# Mount Google Drive\n",
        "GoogleColab may close your GPU Colaboratory session early during peak hours, or after the maximum time limit (about 10-12 hours). If this happens you may lose the model you have finetuned. If we save it directly to a Google Drive we can save it as we finetune it.\n",
        "\n",
        "This step mounts your Google Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btGNo-A4-b6-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oH5qmese-fUy"
      },
      "source": [
        "# Copy training files from Google Drive\n",
        "Before running this step, upload the training files directly to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2xgnRCk-fwu"
      },
      "source": [
        "full_path = '/content/drive/My Drive/' + training_file\n",
        "!cp \"$full_path\" \"/content\"\n",
        "\n",
        "full_path = '/content/drive/My Drive/' + eval_file\n",
        "!cp \"$full_path\" \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnRX3yBQYwNQ"
      },
      "source": [
        "# Set a label for this finetuning task\n",
        "Set a directory for this finetuning task. It helps to use something more distinctive.\n",
        "\n",
        "I like to use `bot_ddmmyyyy` as a crude form of version control."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C_y3YXyZR-e"
      },
      "source": [
        "bot_label = 'bot_27102020'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhxmAVugZsmq"
      },
      "source": [
        "# Start the finetuning process\n",
        "\n",
        "Unfortunately Google have recently stopped serving Tesla T4 GPUs on the free Colab. You will likely get a K80 GPU which is a lot slower for fine tuning GPT-2.\n",
        "\n",
        "The code below will adapt for K80 or Tesla T4.\n",
        "\n",
        "On the K80 GPU and with ~10-12Mb training data text file, it will train for around 6 epochs (loops of all training data). Each epoch will take 60-80 minutes so it is likely you will get kicked out of Colab before finishing the training.\n",
        "\n",
        "If you have been kicked off before finishing all epochs, open a new Colab session when you can and run all these steps again. The final step will attempt to detect the existing best_model and resume training from that point. It will restart the epochs from 0 so you might need to keep track of exactly how many have been done, or keep track of the eval_loss and stop when it starts rising.\n",
        "\n",
        "The score at each training save point will be saved in a file called training_progress_scores.csv. The save point with the lowest eval_loss is the best one to use. After that, the eval_loss will begin to increase and the model will start to be over-trained/over-fit.\n",
        "\n",
        "The save point with the lowest eval_loss will be automatically saved in the best_model/ folder. You should download and use this one for your chatbot.\n",
        "\n",
        "More training is not necessarily better as it can lead to overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moPPDrgjf0Z6"
      },
      "source": [
        "from simpletransformers.language_modeling import LanguageModelingModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "\n",
        "# Switch to the Google Drive directory\n",
        "%cd \"/content/drive/My Drive/\"\n",
        "\n",
        "args = {\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"learning_rate\": 1e-4,\n",
        "    # larger batch sizes will use more training data but consume more ram\n",
        "    # accumulation steps\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "\n",
        "    # Use text because of grouping by reddit submission\n",
        "    \"dataset_type\": \"simple\",\n",
        "    # Sliding window will help it manage very long bits of text in memory\n",
        "    \"sliding_window\": True,\n",
        "    \"max_seq_length\": 512,\n",
        "\n",
        "\t\t\"mlm\": False, # has to be false for gpt-2\n",
        "\n",
        "    \"evaluate_during_training\": True,\n",
        "    # default 2000, will save by default at this step.\n",
        "    # \"evaluate_during_training_steps\": 2000,\n",
        "    \"use_cached_eval_features\": True,\n",
        "    \"evaluate_during_training_verbose\": True,\n",
        "\n",
        "    # don't save optimizer and scheduler we don't need it\n",
        "    \"save_optimizer_and_scheduler\": False,\n",
        "    # Save disk space by only saving on checkpoints\n",
        "    \"save_eval_checkpoints\": True,\n",
        "    \"save_model_every_epoch\": False,\n",
        "    # disable saving each step to save disk space\n",
        "    \"save_steps\": -1, \n",
        "\n",
        "    \"output_dir\": f\"{bot_label}/\",\n",
        "\t\t\"best_model_dir\": f\"{bot_label}/best_model\",\n",
        "}\n",
        "\n",
        "if 'K80' in torch.cuda.get_device_name(0):\n",
        "  # Most of the time we'll only get a K80 on free Colab\n",
        "  args['train_batch_size'] = 1\n",
        "  # Need to train for multiple epochs because of the small batch size\n",
        "  args['num_train_epochs'] = 6\n",
        "  args[\"gradient_accumulation_steps\"] = 100\n",
        "  # Save every 3000 to conserve disk space\n",
        "  args[\"evaluate_during_training_steps\"] = int(3000 / args[\"gradient_accumulation_steps\"])\n",
        "\n",
        "elif 'T4' in torch.cuda.get_device_name(0):\n",
        "  # You may get a T4 if you're using Colab Pro\n",
        "  # larger batch sizes will use more training data but consume more ram\n",
        "  args['train_batch_size'] = 8\n",
        "  # On Tesla t4 we can train for steps rather than epochs because of the batch size\n",
        "  args[\"max_steps\"] = 12000\n",
        "  # default 3000, will save by default at this step.\n",
        "  args[\"evaluate_during_training_steps\"] = 3000,\n",
        "\n",
        "# Check to see if a model already exists for this bot_label\n",
        "resume_training_path = f\"/content/drive/MyDrive/{bot_label}/best_model/\"\n",
        "\n",
        "if os.path.exists(resume_training_path):\n",
        "    # A model path already exists. So we'll attempt to resume training starting fom the previous best_model.\n",
        "    args['output_dir'] = resume_training_path\n",
        "    args['best_model_dir'] = f\"{resume_training_path}/resume_best_model/\"\n",
        "    model = LanguageModelingModel(\"gpt2\", resume_training_path, args=args)\n",
        "\n",
        "else:\n",
        "  # Create a new model\n",
        "  model = LanguageModelingModel(\"gpt2\", \"gpt2\")\n",
        "\n",
        "model.train_model(train_file=training_file, eval_file=eval_file, args=args, verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJXaoD6uKY93"
      },
      "source": [
        "## Complete!\n",
        "The model is now finetuned.\n",
        "Go back to your Google Drive, download the model and unzip it into the `models` folder in the ssi-bot project. "
      ]
    }
  ]
}